{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dnude example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from datetime import date\n",
    "import re\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_version = \"2017_06_22_more_mapped\"\n",
    "# analysis_version = \"2017_06_26\"\n",
    "# analysis_version = \"2017_06_29\"\n",
    "analysis_version = \"prepared_for_conversation\"\n",
    "project_dir_absolute = Path('/data/Dnude/francois') # needs to be pathlib.Path object\n",
    "# project_dir_absolute = Path('~/data/francois') # needs to be pathlib.Path object\n",
    "project_dir_absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generic project structure\n",
    "project_dir = Path(project_dir_absolute.name)\n",
    "dicoms_dir = Path('anal/dicoms') # place dicoms at this location\n",
    "scripts_dir  = Path('anal/heudiconv_files')\n",
    "heuristics_script = scripts_dir.joinpath('heuristics/fmrif_heuristics_' + analysis_version + '.py')\n",
    "default_heuristics = heuristics_script.with_name('convertall.py')\n",
    "\n",
    "sing_image = scripts_dir.joinpath('singularity_images/nipy_heudiconv-2017-05-27-2471285b9681.img')\n",
    "\n",
    "dicom_extension = '.tar'\n",
    "# path inside container\n",
    "outdir = Path(\"/data/bids_\" + analysis_version) \n",
    "# on local filesystem\n",
    "output_of_heudiconv = Path(outdir.name) \n",
    "\n",
    "outdir_gen = Path(outdir.as_posix() + '_generic')\n",
    "output_of_heudiconv_gen = Path(outdir_gen.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pwd\n",
    "%cd {project_dir_absolute.as_posix()}\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda environment used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!conda env export > {scripts_dir.joinpath('conda_env.yml')} \n",
    "# this was subsequently edited to remove some package versions that don't exist on osx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The conda environment used for this analysis can be recreated using the above yml file and the command :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conda env create -f conda_env.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bids_ses(df):\n",
    "    df = df.assign(bids_ses = ['{num:02d}'.format(num = 1 + i) for i in range(len(df))])\n",
    "    return df\n",
    "\n",
    "df_dicoms = pd.DataFrame({'dicom_path' : [p.as_posix() for p in dicoms_dir.glob('*' + dicom_extension)]})\n",
    "\n",
    "# the layout of the filename matters \n",
    "# for the construction of the singularity command later on.\n",
    "df_dicoms = pd.concat(\n",
    "    [df_dicoms,\n",
    "     df_dicoms.dicom_path.\n",
    "     str.extract(\n",
    "         '.*-(?P<patient_id>[0-9]{,8})-(?P<date>[0-9]{8})-(?P<exam_id>[A-Z0-9]*)-.*' + dicom_extension,\n",
    "         expand=True)],\n",
    "    axis = 1)\n",
    "\n",
    "\n",
    "df_subs = df_dicoms.drop_duplicates('patient_id')[['patient_id']]\n",
    "df_subs = df_subs.assign(bids_subj = ['{x:03d}'.format(x=x) for x in range(1, 1 + len(df_subs))])\n",
    "df_subs\n",
    "df_dicoms = pd.merge(df_dicoms,df_subs,on='patient_id')\n",
    "df_bids = (\n",
    "    df_dicoms[['dicom_path','bids_subj','date','exam_id','patient_id']].\n",
    "    assign(exam_id_as_int = lambda df: df.exam_id.astype(int)).\n",
    "    sort_values(['bids_subj','exam_id_as_int']).\n",
    "    groupby(\n",
    "         ['bids_subj'],\n",
    "         as_index = False).\n",
    "     apply(add_bids_ses)\n",
    ")\n",
    "df_bids['dicom_template'] = \\\n",
    "'/data/' + dicoms_dir.as_posix() + \\\n",
    "'/*-' + df_bids.patient_id + '-' + \\\n",
    "df_bids.date + '-' + df_bids.exam_id + \\\n",
    "'-DICOM' + dicom_extension\n",
    "df_bids.dicom_template\n",
    "\n",
    "df_bids.to_csv(dicoms_dir.joinpath('bids_mapping' +analysis_version + '.tsv'),sep='\\t')\n",
    "\n",
    "df_bids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_bids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running heudiconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for generating singularity commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_singularity_command(df_row,project_dir=None,heuristics_script=None,output_dir=None,conv_dir=None,\\\n",
    "                                 anon_script=None,conversion=False,minmeta=False, sing_image=None):\n",
    "    heuristics_script = Path('/data').joinpath(heuristics_script).as_posix()\n",
    "    project_dir = Path(project_dir).as_posix()\n",
    "    output_dir = Path(output_dir).as_posix()\n",
    "    cmd = 'module load singularity;' + \\\n",
    "    'singularity exec' + \\\n",
    "    ' --bind ' + project_dir + ':/data' + \\\n",
    "    ' ' + sing_image.as_posix() + \\\n",
    "    ' heudiconv' + \\\n",
    "    ' -d ' + df_row.dicom_template + \\\n",
    "    ' -s ' + df_row.bids_subj + \\\n",
    "    ' -ss ' + df_row.bids_ses + \\\n",
    "    ' -f ' + heuristics_script + \\\n",
    "    ' -b' \n",
    "    \n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir).as_posix()\n",
    "        cmd = cmd + ' -o ' + output_dir \n",
    "\n",
    "    if conv_dir is not None:\n",
    "        conv_dir = Path(conv_dir).as_posix()\n",
    "        cmd = cmd + ' --conv-outdir ' + conv_dir \n",
    "        \n",
    "    if minmeta:\n",
    "        cmd = cmd + ' --minmeta'\n",
    "\n",
    "    if conversion:\n",
    "        cmd = cmd + ' -c dcm2niix' \n",
    "    else:\n",
    "        \n",
    "        cmd = cmd + ' -c none' \n",
    "        \n",
    "    return cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run heudiconv without conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate swarm commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sing = (\n",
    "    df_bids.\n",
    "    assign(\n",
    "        cmd = lambda df:\n",
    "        generate_singularity_command(df,\n",
    "                                     project_dir = project_dir_absolute,\n",
    "                                     output_dir=outdir_gen,\n",
    "                                     heuristics_script= default_heuristics,\n",
    "                                     conversion = False,\n",
    "                                     sing_image = sing_image))\n",
    "          )\n",
    "\n",
    "swarm_path_gen = Path(scripts_dir).joinpath('heudiconv_generic_swarm' + analysis_version + '.cmd')\n",
    "\n",
    "# not all commands resolve to a single dicom so getting unique ones before writing swarm\n",
    "swarm_path_gen.write_text('\\n'.join(df_sing.cmd.drop_duplicates())) \n",
    "print(swarm_path_gen.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_gen = scripts_dir.joinpath('swarm_output', analysis_version +'_generic')\n",
    "log_dir_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_gen = !swarm -f {swarm_path_gen} -g 10 --logdir {log_dir_gen} --partition nimh,norm\n",
    "job_id_gen = job_id_gen[0]\n",
    "job_id_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information obtained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe displayed below shows the information from the dicom headers. This information can be used by a custom heuristics file to convert dicoms to specific modalities in the BIDS structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_text_paths_gen = list(output_of_heudiconv_gen.glob('**/info/dicominfo.txt'))\n",
    "info_text_paths_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_gen = pd.concat([pd.read_csv(p, sep = '\\t').assign(file_path=p) for p in info_text_paths_gen]).reset_index(drop = True)\n",
    "df_info_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_info_gen.series_description.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Checking the swarm output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes specific runs fail (observed on dashboard). File these in as 'files of interest':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_of_interest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_files_paths = pd.DataFrame([x.as_posix() for x in log_dir_gen.glob('*.e')],columns=['paths'])\n",
    "df_error_files_paths = (df_error_files_paths.\n",
    "                  loc[df_error_files_paths.paths.str.find(job_id_gen)>0,:].\n",
    "                  assign(run = lambda df:\n",
    "                         df.paths.str.extract(\n",
    "                             '/.*swarm_\\d*_(\\d*).e',\n",
    "                             expand=False).astype(int)).\n",
    "                  sort_values('run'))\n",
    "df_error_files_paths\n",
    "\n",
    "if not files_of_interest:\n",
    "    files_of_interest = list(range(len(df_error_files_paths)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n\\n'.join(np.array(df_error_files_paths.paths)[files_of_interest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_files = [Path(x).read_text() for x in np.array(df_error_files_paths.paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n\\n'.join(error_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_files = [Path(x).with_suffix('.o').read_text() for x in np.array(df_error_files_paths.paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n\\n'.join(output_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_dicoms.query('bids_subj in [\"324\",\"108\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# module load afni\n",
    "# dicom_hdr ???.dcm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heudiconv with custom heuristics script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heudiconv requires a heuristics file (created in the next section) in order to map the dicom files' metadata to the bids output structure. This is documented at the  nipy/heudiconv github repository. The file contains two main parts:\n",
    "1. Templates create using the \"create_key\" function that specify where each run type belongs\n",
    "2. The specification of the heuristic to categorise each run in the dicom tar.\n",
    "\n",
    "The template is quite stereotyped and the examples on github are useful in figuring out how to write them.\n",
    "\n",
    "The heuristic for categorising the runs is a little more challenging. Often the series description from the dicom header can be enough to categorise the scans:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_descriptions = set(df_info_gen.series_description)\n",
    "series_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heuristic is written as a boolean python expression that queries \"s\" a named tuple with a number of fields that are extracted from the dicom header. Here we will triple quote the heuristics so that we can pass the expression as a string but this is not required in our heuristics file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mprage_heuristic = \"\"\"(' MP-Rage 1 mm' == s.series_description)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_heuristic(s,heuristic):\n",
    "    return eval(heuristic)\n",
    "\n",
    "def check_heuristic_df(df,heuristic):\n",
    "    matching = []\n",
    "    for s in df.itertuples():\n",
    "        result = check_heuristic(s,heuristic)\n",
    "        if result:\n",
    "            matching.append(s)\n",
    "    return pd.DataFrame(matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_heuristic_df(df_info_gen, mprage_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memprage = \"\"\"('ME-MP-RAGE 1mm PROMO' == s.series_description)\"\"\"\n",
    "check_heuristic_df(df_info_gen, memprage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti = \"\"\"(s.series_description.startswith('edti'))\"\"\"\n",
    "check_heuristic_df(df_info_gen, dti)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fmrif heuristics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristics_script.write_text(\"\"\"\n",
    "# coding: utf-8\n",
    "import os\n",
    "\n",
    "\n",
    "def create_key(template, outtype=('dicom', 'nii.gz'), annotation_classes=None):\n",
    "    if template is None or not template:\n",
    "        raise ValueError('Template must be a valid format string')\n",
    "    return template, outtype, annotation_classes\n",
    "\n",
    "def infotodict(seqinfo):\n",
    "\n",
    "    mprage = create_key('anat/sub-{subject}_{session}_acq-MPRAGE_run-{item:03d}_T1w')\n",
    "    flair = create_key('anat/sub-{subject}_{session}_run-{item:03d}_FLAIR')\n",
    "    MEMPRAGE1mmPROMO = create_key('anat/sub-{subject}_{session}_acq-MEMPRAGE1mmPROMO_run-{item:03d}_T1w')\n",
    "    t1SE  = create_key('anat/sub-{subject}_{session}_acq-SE_run-{item:03d}_T1w')\n",
    "    t2fatsat = create_key('anat/sub-{subject}_{session}_acq-fatsat_run-{item:03d}_T2w')\n",
    "    FRFSE = create_key('anat/sub-{subject}_{session}_acq-FRFSE_run-{item:03d}_T2w')\n",
    "    CUBE = create_key('anat/sub-{subject}_{session}_acq-CUBE_run-{item:03d}_T2w')\n",
    "    rest = create_key('func/sub-{subject}_{session}_task-rest_run-{item:02d}_bold')\n",
    "    dti = create_key('dwi/sub-{subject}_{session}_run-{item:02d}_dwi')\n",
    "    ref = create_key('anat/sub-{subject}_{session}_acq-ref_fr8_run-{item:03d}_T1w')\n",
    "    \n",
    "    info = {mprage: [], MEMPRAGE1mmPROMO: [], t1SE: [],\n",
    "    t2fatsat: [], flair: [], FRFSE: [], rest:[],\n",
    "    CUBE:[],dti: [], ref: []}\n",
    "\n",
    "   \n",
    "    for s in seqinfo:\n",
    "        if ('Ax T2 FLAIR' == s.series_description):\n",
    "             info[flair].append(s.series_number)\n",
    "        if ('Ax T2 FRFSE' == s.series_description):\n",
    "             info[FRFSE].append(s.series_number)\n",
    "        if ('FMRIF EPI 3mm iso RS' == s.series_description):\n",
    "             info[rest].append(s.series_number)\n",
    "        if ('ME-MP-RAGE 1mm PROMO' == s.series_description):\n",
    "             info[MEMPRAGE1mmPROMO].append(s.series_number)\n",
    "        if (' MP-Rage 1 mm' == s.series_description):\n",
    "             info[mprage].append(s.series_number)\n",
    "        if ('Sag CUBE T2' == s.series_description):\n",
    "             info[CUBE].append(s.series_number)\n",
    "        if ('Sag T1 Spin echo' == s.series_description):\n",
    "             info[t1SE].append(s.series_number)\n",
    "        if ('T2_1.7mm_fat_sat' == s.series_description):\n",
    "             info[t2fatsat].append(s.series_number)\n",
    "        if (s.series_description.startswith('edti')):\n",
    "            info[dti].append(s.series_number)\n",
    "        if ('Sagittal Ref PA fr8' == s.series_description):\n",
    "             info[ref].append(s.series_number)\n",
    "        if ('Sagittal Ref body fr8' == s.series_description):\n",
    "            info[ref].append(s.series_number)\n",
    "    return info\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate heudiconv swarm commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sing = (\n",
    "    df_bids.\n",
    "    assign(\n",
    "        cmd = lambda df:\n",
    "        generate_singularity_command(df,\n",
    "                                     project_dir = project_dir_absolute,\n",
    "                                     output_dir=outdir,\n",
    "                                     heuristics_script= heuristics_script,\n",
    "                                     conversion = True,\n",
    "                                     sing_image = sing_image))\n",
    "          )\n",
    "\n",
    "swarm_path= Path(scripts_dir).joinpath('heudiconv_swarm' + analysis_version + '.cmd')\n",
    "\n",
    "# not all commands resolve to a single dicom so getting unique ones before writing swarm\n",
    "swarm_path.write_text('\\n'.join(df_sing.cmd.drop_duplicates())) \n",
    "print(swarm_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(df_sing.cmd),len(df_sing.cmd.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run heudiconv conversion swarm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = scripts_dir.joinpath('swarm_output', analysis_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = !swarm -f {swarm_path} -g 10 --logdir {log_dir} --partition nimh,norm\n",
    "job_id = job_id[0]\n",
    "job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Issues with conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swarm failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some files failed (observed on dashboard). File these in as 'files of interest':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_of_interest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_files_paths = pd.DataFrame([x.as_posix() for x in log_dir.glob('*.e')],columns=['paths'])\n",
    "\n",
    "df_error_files_paths = (df_error_files_paths.\n",
    "                  loc[df_error_files_paths.paths.str.find(job_id)>0,:].\n",
    "                  assign(run = lambda df:\n",
    "                         df.paths.str.extract(\n",
    "                             '/.*swarm_\\d*_(\\d*).e',\n",
    "                             expand=False).astype(int)).\n",
    "                  sort_values('run'))\n",
    "\n",
    "df_error_files_paths\n",
    "\n",
    "if not files_of_interest:\n",
    "    files_of_interest = list(range(len(df_error_files_paths)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n\\n'.join(np.array(df_error_files_paths.paths)[files_of_interest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_files = [Path(x).read_text() for x in np.array(df_error_files_paths.paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n\\n'.join(error_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_files = [Path(x).with_suffix('.o').read_text() for x in np.array(df_error_files_paths.paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n\\n'.join(output_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output bids directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -Rl {output_of_heudiconv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list(output_of_heudiconv.glob('**/sub-002*MEMPRAGE1mmPROMO_run-001_T1w*json')):\n",
    "    !echo {f}\n",
    "    !cat {f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %cp -R {output_of_heudiconv} {output_of_heudiconv.with_name('BIDS')}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "67px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1148px",
    "left": "0px",
    "right": "auto",
    "top": "105px",
    "width": "257px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
